\chapter{Our Solution}
In our solution we focus on design of a black-box adversarial attack, for TODO reason. We will shortly elaborate on available black-box attack strategies and then we will present and discuss our pure evolutionary algorithm based approach and finally suggest a hybrid method employing evolutionary algorithm with surrogate model as an informed operator.

\section{Related Work}
Regarding the black-box adversarial attacks for deep learning models, only a few methods has been proposed so far. \cite{DBLP:journals/corr/PapernotMGJCS16} suggest an approach converting black-box attack to surrogate attack, by synthesizing dataset for the surrogate model to be trained on. \cite{8014906} performs local-search of the neighbourhood of targeted image to estimate the gradient of targeted model. This estimate is later use to guide the generation of adversarial example. The former approach is later extended by \cite{DBLP:journals/corr/abs-1804-08598}, who employ natural evolutionary strategies to estimate target model gradients with respect to targeted image. The adversarial example is generated using the gradient descent algorithm on the estimated gradient. Finally, \cite{Vidnerova:2016:EGA:2955129.2955178} used evolutionary algorithms to perform adversarial attack mostly on shallow machine learning models or models based -- we will base our solution on their approach, evaluating the method on state of the art deep learning models and proposing a multitude of improvements to the evolutionary algorithm based strategy.

\section{Pure Evolutionary Algorithms}
TODO why evolution and for example beam search

As stated above, the method we will be improving is based on use of evolutionary algorithm. \cite{Vidnerova:2016:EGA:2955129.2955178} propose following configuration of the evolutionary algorithm. For the targeted model $M_\theta$, with input $[0,1]^{h \times w}$. The individual $i$ is encoded as a vector of pixel intensities ($i \in [0, 1]^{h w}$). The two-point crossover is selected as a reproduction operator and for the mutation, the biased random mutation with Gaussian distribution is employed. Selection is performed using tournament selection of size 3 and relies on fitness function which is an arithmetical average of Euclidean distance between source image and individual (adversarial example intensity) and Euclidean distance between prediction of individual under $M_\theta$ and targeted predictions. The evaluation, performed on several shallow models trained on MNIST dataset, runs $10,000$ generations of evolutionary algorithm with population size of 50, allowing for at most $500,000$ accesses of the targeted model.

We propose the change of encoding if the individual to \emph{2D} matrix to maintain spatial relations of data -- images. Those relations are further exploited by selected operators, namely cross-over. The suitability of this change is supported by the fact, that vast majority of deep learning image classifiers is based on use of convolutional layers, which extracts the local context of processed data. We also experimented with perturbations smaller in terms of shape, which where added to the targeted image using bilinear interpolation, while proving moderatly successful, they did not outperformed full sized scenario.

The use of two point cross-over is preserved with necessary changes to support data of higher dimensionality. In our case the cross-over swaps randomly selected rectangular regions of parent individuals i.e., image crops, to produce offspring. We experimented with the use of universal crossover without success -- we assume, that the degradation in performance is again caused by the inherent properties of convolutional layers, where the partial change of area covered by convolutional kernel yields unstable results in convolutional layer activation, while the change of whole rectangular region maintains the local response of the layer.

The idea of biased mutation is preserved as well, but we change the probabilty distribution to binomial distribution i.e., discrete probability distribution, maintaining the discrete characteristics of intensity of pixels. As a result, biased mutation driven by binomial distribution mimics shifting of the intensity of the pixel by several shades of gray, while at the same time maintaining the beneficial properties of normal distribution.

For the optimization we use two objectives -- the score of prediction of targeted class, which needs to overcome 0.5 for successful attack; and MSE of individual with respect to targeted image, to quantify the intensity of perturbation. We choose MSE over Euclidean distance for it account more for big pixel-wise differences.

As a result, the selection operator is changed to selection suitable for multi-objective optimization, namely non-dominated sorting algorithm. For that matter, we choose \emph{NSGA-II} with crowding distance, which uses concept of Pareto-optimality to construct a so called \emph{non-dominated} fronts -- gropus of mutually non-dominated individuals. Furthermore NSGA-II employs secondary sorting algorithm (in our case crowding distance) allowing for more stable selection of individuals (see \cite{nsga} for more). We assume this approach superior to average of objectives for the issues with different scaling of prediction error and perturbation intensity. Appart from that, we suggest the use of clipping on the intensity of perturbation if it overcome certain admissible value, combining the \emph{constrained} and \emph{optimized} adversarial attack paradigm.

Finaly the dual clipping takes place. Firstly, to maintain the individual in the range of a unit cube (the searched space) and secondly to maintaint the pixel-wise perturbation in the allowed range -- this is employed to reject individuals with few significantly shifted pixels, as those do not show sufficient visual similarity to targeted image.

We evaluate our solution on the Fashion MNIST dataset, allowing each run of evolutionary algorithm to perform at most $~65000$ evaluations of targeted model $M_\theta$, we chose this amount to get comparable results with white-box and surrogate approaches trained on training set of Fashion MNIST with size $60,000$. See section \ref{sec:experiments} for results and comparison.

TODO our implementation? as an attachment -- rather not??

\section{Hybrid Methods}
In addition to our pure evolutionary algorithm based approach, we suggest an extension combining black-box and surrogte approach. Therefore we define a new operator \emph{FGSMMutation}, using surrogate model and corresponding gradients obtained with fast gradient sign method as an informed mutation operator.
The newly defined operator mutates the individual with respect to the estimate of the gradient, forcing the pixel-wise change sign to correspond with the gradient sign. The magnitude of change may either be fixed or sampled from a non-negative probability distribution e.g., truncated normal or truncated binomial. We recognize two possible origins of surrogate model. Firstly, a \emph{pretrained} surrogate model may be provided during the initialization of the operator -- this approach may be suitable for underperforming surrogates to boost their performance with the help of evolutionary algorithm. 

