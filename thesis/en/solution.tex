\chapter{Our Solution}
\label{sec:solution}
In our solution we focus on design of a black-box adversarial attack, for this is the likely real life scenario, as the number of web services offering image classification is soaring and image classificators are being embedded into various devices e.g., security cameras. We will shortly elaborate on available black-box attack strategies and then we will present and discuss our pure genetic algorithm based approach and finally suggest a hybrid method employing genetic algorithm with surrogate model as an informed operator.

\section{Related Work}
Regarding the black-box adversarial attacks for deep learning models, only a few methods has been proposed so far. \cite{DBLP:journals/corr/PapernotMGJCS16} suggest an approach converting black-box attack to surrogate attack, by synthesizing dataset for the surrogate model to be trained on. \cite{8014906} perform local-search of the neighbourhood of targeted image to estimate the gradient of targeted model. This estimate is later use to guide the generation of adversarial example. The former approach is later extended by \cite{DBLP:journals/corr/abs-1804-08598}, who employ natural evolutionary strategies to estimate target model gradients with respect to targeted image. The adversarial example is generated using the gradient descent algorithm on the estimated gradient. Finally, \cite{Vidnerova:2016:EGA:2955129.2955178} used genetic algorithms to perform adversarial attack mostly on shallow machine learning models. We will base our solution on their approach, evaluating the method on the state of the art deep learning models and proposing a multitude of improvements to the genetic algorithm based strategy.

\section{Evolutionary Generated Adversarial Examples}
\label{sec:pga}
Analysing approaches towards generation of adversarial examples in black-box scenarios, we chose evolutionary algorithms -- particularly genetic algorithms, as promising candidates. Unlike other heuristic random space search algorithms, genetic algorithms exploits the inherent property of the way that deep learning models process imagery data. Using reproduction operators, individuals in population interact with each other, allowing for recombination of promising genes. As the deep convolutional neural networks extract the information from the local context using convolutional layers, we assume that by combining parts of promising adversarial examples, we can obtain ones scoring even better results.

As already stated, the method we propose is based on use of genetic algorithms by \cite{Vidnerova:2016:EGA:2955129.2955178}. They propose the following configuration of the solution. For the targeted model $M_\theta$, with input $[0,1]^{h \times w}$, the individual $i$ is encoded as a vector of pixel intensities ($i \in [0, 1]^{h w}$). The two-point crossover is selected as a reproduction operator, and the biased random mutation with Gaussian distribution is employed. Selection is performed using tournament selection of size 3 and relies on fitness function which is an arithmetical average of $l2$ norm of perturbation intensity and $l2$ distance between prediction of individual under $M_\theta$ and targeted prediction. The evaluation, performed on several shallow models trained on MNIST dataset, runs $10\,000$ epochs of genetic algorithm with population size of 50, allowing for at most $500\,000$ accesses to the targeted model (see section \ref{sec:ga_theory} on genetic algorithms theory).

In our solution, we propose the change of encoding of the individual to \emph{2D} matrix to maintain spatial relations of the data -- images. Those relations are further exploited by selected operators, namely cross-over. The suitability of this change is supported by the fact, that vast majority of deep learning image classifiers is based on use of convolutional layers, which extracts the spatial context of processed data. We have also experimented with generating perturbations smaller in terms of shape, which were added to the targeted image using bilinear interpolation. While proving moderatly successful, they did not outperformed full sized scenario. Our individual is thus defined as a matrix $i \in [0, 1]^{h \times w}$ and represents perturbation $\eta$. We use zero centered binomial distribution scaled by factor of $\frac{1}{255}$ to sample initial population.

The use of two point cross-over is preserved with necessary changes to support the individuals of higher dimensionality. In our case the cross-over swaps randomly selected rectangular regions i.e., image crops of parent individuals, to produce offspring (see fig \ref{fig:xover}). We experimented with the use of universal crossover without success -- we assume, that the degradation in performance is again caused by the inherent properties of convolutional layers, where the partial change of area covered by convolutional kernel yields unstable results in convolutional layer activation, while the change of whole rectangular region maintains the expected local response of the layer.

\begin{figure}
    \centering
    \includegraphics[width=.64\textwidth]{../img/xover.pdf}
    \caption{Two-point cross-over on images}
    \label{fig:xover}
\end{figure}

The idea of biased mutation is preserved as well, but we change the probability distribution to binomial distribution i.e., discrete probability distribution maintaining the discrete characteristics of intensity of pixels. As a result, biased mutation driven by zero centered binomial distribution mimics shifting of the intensity of the pixel by several shades of gray to either direction, while at the same time maintaining the beneficial properties of normal distribution. The binomial mutation (Mut) of individual $i$, with binomial distribution $\mathcal{B}(n, p)$, has the form of (\ref{eqn:bin_mut}).

\begin{equation} \label{eqn:bin_mut}
\text{Mut}(i)_{x,y} =
\begin{cases}
    i_{x, y} + \frac{b}{255}, & \text{with probability } p_{gene} \text{, where } b \sim \mathcal{B}(n, p) - \frac{n}{2}\\
    i_{x, y}, & \text{with probability } 1 - p_{gene}
\end{cases}
\end{equation}

For the optimization we use two objectives -- the score of prediction of targeted class, which needs to overcome 0.5 for successful attack; and $l2$ norm of~perturbation (individual) $\eta = i$ (\ref{eqn:l2}), to quantify the intensity of it. We assume $l2$ norm being promising option for it accounts more for big pixel-wise differences. Appart from that, we suggest the use of clipping on the intensity of perturbation if it gets below certain admissible value $c_\text{bound}$ i.e., taking the $\max(c_\text{bound}, l2(i))$ as a perturbation intensity, combining the \emph{constrained} and \emph{optimized} adversarial attack paradigm.

\begin{equation} \label{eqn:l2}
l2(i) = \sqrt{\sum\limits_x\sum\limits_y i_{x, y}^2}
\end{equation}

As a result, the selection operator is changed to selection suitable for multi-objective optimization, namely non-dominated sorting algorithm. For that matter, we choose \emph{NSGA-II} with crowding distance, which uses concept of Pareto-optimality to construct a so called \emph{non-dominated fronts} -- groups of mutually non-dominated individuals. Furthermore NSGA-II employs secondary sorting algorithm (in our case crowding distance) allowing for more stable selection of individuals (see \cite{nsga} for more). We assume this approach superior to average of objectives for the issues with different scaling of prediction error and perturbation intensity (see algorithm \ref{algo:nsga_cd}). The new population is selected from the union of parents and offspring of the current generation.

\begin{algorithm}
\caption{NSGA operator}
\label{algo:nsga_cd}
\begin{algorithmic}
\STATE $P_{in} \gets \text{input population}$
\STATE $s_{out} \gets \text{target size of selected population}$
\STATE
\STATE $P_{out} \gets \text{empty list}$
\WHILE{$|P_{out}| < s_{out}$}
\STATE $f \gets \text{pop non dominated individuals from } P$
\IF{$|P_{out}| + |f| <= s_{out}$}
\STATE extend $P_{out}$ with $f$
\ELSE
\STATE sort $f$ by crowding distance
\STATE extend $P_{out}$ with first $s_{out} - |P_{out}|$ elements of $f$
\ENDIF
\ENDWHILE
\RETURN $P_{out}$
\end{algorithmic}
\end{algorithm}

Finally, the dual clipping takes place before evaluation of objectives. Firstly, to maintain the individual in the range of a unit cube (the searched space) and secondly to maintain the pixel-wise perturbation in the allowed range -- this is employed to reject individuals with few significantly shifted pixels, as those do not show sufficient visual similarity to targeted image. Clipping (Clip) with lower bound $l$ and upper bound $u$ for individual $i$ corresponds to (\ref{eqn:clip}), clipping of the whole population clips each individual.

\begin{equation} \label{eqn:clip}
\text{Clip}_{(l,u)}(i)_{x, y} = \max\left(l, \min(u, i_{x, y})\right)
\end{equation}

\begin{algorithm}
\caption{Evolutionary generated adversarial examples}
\label{algo:evgena}
\begin{algorithmic}
\STATE $M_\theta \gets \text{targeted model}$
\STATE $x \gets \text{targeted image, of shape } h \times w,\ l' \gets \text{targeted class}$
\STATE $s_{pop} \gets$ population size, $n_{epoch} \gets$ epoch count upper bound
\STATE $c_\text{bound} \gets$ perturbation intensity clipping value
\STATE $\Delta_{max} \gets$ max pixel-wise difference bound
\STATE
\STATE $P \gets \left[\frac{1}{255}\left(\mathcal{B}(n, p) - \frac{n}{2}\right)\right]^{h \times w}$
\WHILE{\NOT $\exists i \in P: M_\theta(i) = l'$}
\STATE $P_{old} \gets P$
\STATE shuffle $P$
\STATE $P \gets \text{Xover}(P)$
\STATE $P \gets \text{Mut}(P)$
\STATE $P \gets \text{Clip}_{(-\Delta_{max},\Delta_{max})}(P)$
\STATE objectives evaluation $\forall i \in P: \left(
    M_\theta \left(\text{Clip}_{(0,1)}\left(x + i\right) \right)[l'],
    \max \left(c_\text{bound}, l2\left(i\right) \right)
\right)$
\STATE $P = \text{NSGA}\left([P, P_{old}], s_{pop}\right)$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

The whole proposed strategy towards generating adversarial examples takes form of algorithm \ref{algo:evgena}. We provide our implementation as an open-sourced project under MIT license -- see \cite{git_evgena} github repository.

We evaluate our solution on the Fashion MNIST dataset, allowing each run of genetic algorithm to perform at most $\sim 65\,000$ evaluations of targeted model $M_\theta$, we choose this amount to get comparable results with white-box and surrogate approaches trained on training set of Fashion MNIST of size $60\,000$. See section \ref{sec:blackbox_ga} for results and comparison.
