\chapter{Our Solution}
\label{sec:solution}
In our solution we focus on design of a black-box adversarial attack, for TODO reason. We will shortly elaborate on available black-box attack strategies and then we will present and discuss our pure evolutionary algorithm based approach and finally suggest a hybrid method employing evolutionary algorithm with surrogate model as an informed operator.

\section{Related Work}
Regarding the black-box adversarial attacks for deep learning models, only a few methods has been proposed so far. \cite{DBLP:journals/corr/PapernotMGJCS16} suggest an approach converting black-box attack to surrogate attack, by synthesizing dataset for the surrogate model to be trained on. \cite{8014906} performs local-search of the neighbourhood of targeted image to estimate the gradient of targeted model. This estimate is later use to guide the generation of adversarial example. The former approach is later extended by \cite{DBLP:journals/corr/abs-1804-08598}, who employ natural evolutionary strategies to estimate target model gradients with respect to targeted image. The adversarial example is generated using the gradient descent algorithm on the estimated gradient. Finally, \cite{Vidnerova:2016:EGA:2955129.2955178} used evolutionary algorithms to perform adversarial attack mostly on shallow machine learning models. We will base our solution on their approach, evaluating the method on state of the art deep learning models and proposing a multitude of improvements to the evolutionary algorithm based strategy.

\section{Pure Evolutionary Algorithms}
\label{sec:pea}
Analysing approaches towards generation of adversarial examples in black-box scenarios, we chose genetic algorithms -- particularly evolutionary algorithms, as promising candidates. Unlike other heuristic random space search algorithms, evolutionary algorithms exploits the inherent property of the way that deep learning models process imagery data i.e., extracting information from the local context using convolutional layers. Due to this fact, we assume that by combining parts of promising adversarial examples using reproduction operators, we can obtain data with even better results.

As already stated, the method we propose is based on use of evolutionary algorithms by \cite{Vidnerova:2016:EGA:2955129.2955178}. They propose following configuration of the solution. For the targeted model $M_\theta$, with input $[0,1]^{h \times w}$. The individual $i$ is encoded as a vector of pixel intensities ($i \in [0, 1]^{h w}$). The two-point crossover is selected as a reproduction operator and for the mutation, the biased random mutation with Gaussian distribution is employed. Selection is performed using tournament selection of size 3 and relies on fitness function which is an arithmetical average of $l2$ norm of perturbation intensity and $l2$ distance between prediction of individual under $M_\theta$ and targeted prediction. The evaluation, performed on several shallow models trained on MNIST dataset, runs $10\,000$ epochs of evolutionary algorithm with population size of 50, allowing for at most $500\,000$ accesses to the targeted model.

In our solution, we propose the change of encoding of the individual to \emph{2D} matrix to maintain spatial relations of the data -- images. Those relations are further exploited by selected operators, namely cross-over. The suitability of this change is supported by the fact, that vast majority of deep learning image classifiers is based on use of convolutional layers, which extracts the spatial context of processed data. We also experimented with generating perturbations smaller in terms of shape, which were added to the targeted image using bilinear interpolation. While proving moderatly successful, they did not outperformed full sized scenario.

The use of two point cross-over is preserved with necessary changes to support the individuals of higher dimensionality. In our case the cross-over swaps randomly selected rectangular regions i.e., image crops of parent individuals, to produce offspring. We experimented with the use of universal crossover without success -- we assume, that the degradation in performance is again caused by the inherent properties of convolutional layers, where the partial change of area covered by convolutional kernel yields unstable results in convolutional layer activation, while the change of whole rectangular region maintains the expected local response of the layer.

The idea of biased mutation is preserved as well, but we change the probability distribution to binomial distribution i.e., discrete probability distribution maintaining the discrete characteristics of intensity of pixels. As a result, biased mutation driven by binomial distribution mimics shifting of the intensity of the pixel by several shades of gray, while at the same time maintaining the beneficial properties of normal distribution.

For the optimization we use two objectives -- the score of prediction of targeted class, which needs to overcome 0.5 for successful attack; and $l2$ norm of perturbation, to quantify the intensity of it. We assume $l2$ norm being promising option for it accounts more for big pixel-wise differences.

As a result, the selection operator is changed to selection suitable for multi-objective optimization, namely non-dominated sorting algorithm. For that matter, we choose \emph{NSGA-II} with crowding distance, which uses concept of Pareto-optimality to construct a so called \emph{non-dominated fronts} -- gropus of mutually non-dominated individuals. Furthermore NSGA-II employs secondary sorting algorithm (in our case crowding distance) allowing for more stable selection of individuals (see \cite{nsga} for more). We assume this approach superior to average of objectives for the issues with different scaling of prediction error and perturbation intensity. Appart from that, we suggest the use of clipping on the intensity of perturbation if it overcome certain admissible value, combining the \emph{constrained} and \emph{optimized} adversarial attack paradigm.

Finaly the dual clipping takes place. Firstly, to maintain the individual in the range of a unit cube (the searched space) and secondly to maintaint the pixel-wise perturbation in the allowed range -- this is employed to reject individuals with few significantly shifted pixels, as those do not show sufficient visual similarity to targeted image.

We evaluate our solution on the Fashion MNIST dataset, allowing each run of evolutionary algorithm to perform at most $\sim 65\,000$ evaluations of targeted model $M_\theta$, we chose this amount to get comparable results with white-box and surrogate approaches trained on training set of Fashion MNIST of size $60\,000$. See section \ref{sec:experiments} for results and comparison.

TODO our implementation? as an attachment -- rather not??

\section{Hybrid Methods}
\label{sec:hm}
In addition to our pure evolutionary algorithm based approach, we suggest an extension combining black-box and surrogte approach. Therefore we define a new operator \emph{FGSM mutation}, using surrogate model and corresponding gradients obtained with fast gradient sign method as an informed mutation operator. The evaluation of hybrid methods is out of scope of this writing and we leave it only as a suggested direction for improvements.

The newly defined operator mutates the individual with respect to the estimate of the gradient, forcing the pixel-wise change sign to correspond with the gradient sign. The magnitude of change may either be fixed or sampled from a non-negative probability distribution e.g., truncated normal or truncated binomial.

We recognize two possible origins of surrogate model. Firstly, a \emph{pretrained} surrogate model may be provided during the initialization of the operator -- this approach may be suitable for underperforming surrogates to boost their performance with the help of evolutionary algorithm. Secondly, the surrogate model can be trained on the fly using targeted model predictions obtained from evaluation of objective function. We assume the surrogate model to get to estimate the gradients reliably after few epochs, as the training data i.e., the querries to targeted model are localized in small neighbourhood of the population of evolutionary algorithm.

Using the FGSM mutation operator, the surrogate model may be exploited further as a substitute to targeted model in case of evaluation of objective function. We expect setting the policy of evaluation and training of the surrogate model not to be an easy task, which may even prove to be impractical for having too many hyperparameters sensitive to the correct settings.
