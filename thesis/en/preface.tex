\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

The effort to automate various processes in our lives has always been one of the key concepts of scientific research. The automation of mechanical work has already been solved to great extent by advances in engineering. On the contrary, automated processing of information has been solved just partially. Well defined tasks i.e., tasks with fully defined behaviour, can be solved, and the challenge lies just in effectivity of the solution. On the other hand, models solving tasks based on raw real word data, human level input and output or some degree of fuzziness are yet to be found or, if they exist, suffer from several shortcomings. Those tasks form the field of artificial intelligence. Eventhough the artificial intelligence undergone several crisis, not being able to deliver on its promises, it is experiencing boom once again thanks to advances in machine learning.

The ability to learn is believed to be the key of our success, the success of the mankind as a species, and accounts to what we call the intelligence. Following this belief, the machine learning mimic the process of learning by automating extraction of knowledge from experience, with the aim to abstract and grasp the semantics of the observations. This is reached by construction of a mathematical functions, which for given, numericaly represented, observations return valid, numericaly represented, conclusions. Those functions often take form of composite paramteric functions and to solve the task satisfactorily the best possible composition -- architecture, together with the best possible set of parameters is looked for. Current state of the art method in machine learning is deep learning.

The deep learning is based on use of deep neural networks. Neural network is a structure of interconnected artificial neurons, with artificial neuron being unit computing non-linear transformation of linear combination of its inputs, passing this value further to other connected units. With increasing size of those networks in terms of unit count, the convenient abstraction of layers -- mutually dependent groups of (TODO mutually independent) units, emerged. Early neural network based solutions used very few of those layers, mainly due to the hardware limitations, but with the advances in technology, the deep learning i.e., usage of many layered neural networks, was established. Appart from rising complexity of the models new types of layers were invented, namely the convolutional layers extracting the local context from observations. Recurrent neural networks enabling processing of sequences of data were invented as well.

It was the convolutional neural networks i.e., the deep neural networks using convolutional layers, that lift off the new era of image processing, part of the field of computer vision. One of the key tasks solved by computer vision techniques is the task of image classification, which lies in assigning a category from the set of possible categories, to the image. The subjects of those task cover a wide range from classifying real world entities on photographs to medical applications and deciding if the PET/CT scan reports cancerous formations. Usecases with high demands on reliability, like security enforcing systems or aforementioned medical applications, need to be resilient to mistakenly corrupted data or even targeted attacks.

Deep learning image classification solutions reports vulnerability to adversarial examples i.e., artificially created inputs that become misinterpreted. It has been showed that carefully crafted perturbations added to images may successfully lead to model failure, while being unnoticable for the human observer, or a different model. The issue of vulnerability to adversarial attacks does not apply only to the task of image classification, but it also concerns speech recognition, particularly use of personal asistants; or image segmentation and object detection on which autonomous driving depends. It is of great importance to study those vulnerabilities, attacks and corresponding defences to improve reliability of such solutions.

In this thesis, we will cover the task of generating adversarial examples for the models used for classification in computer vision. TODO

The structure of the thesis is as follows. In chapter \ref{sec:preliminaries} the task of image classification is defined, along with overview of available datasets for research purposes (section \ref{sec:cv}). The underlying theory of deep learning for the purpose of image classification (section \ref{sec:deep_learning}), along with theoretical background of evolutionary algorithms (section \ref{sec:ea_theory}), is presented. Chapter \ref{sec:advex} spans the definition of adversarial attacks, their taxonomy (section \ref{sec:advex_taxonomy}) and known attack strategies (sections \ref{sec:advex_gbm} and \ref{sec:advex_gan}). In chapter \ref{sec:solution}, we propose our solution (section \ref{sec:pea}) and its possible extensions (section \ref{sec:hm}). In chapter \ref{sec:experiments}, we analyse known aproaches (sections \ref{sec:whitebox_fgsm} and \ref{sec:surrogate_fgsm}) and compare them to our solution (section \ref{sec:blackbox_ea}), carrying out a multitude of experiments.





% The effort to automate various tasks in our lives has always been one of the most important concepts of scientific research. The inventions of various machines, from the water wheel through the steam engine to the internal combustion engine along with the electric engine, enabled mankind to automate mechanical work. In the same manner, people have always been seeking for ways to automate reasoning.

% The Syllogisms defined by Aristotle are often considered to be the first attempts to automate reasoning. Formation of propositional logic together with predicate logic laid foundations to formal logic. The evolution of formal logic and its parts, namely Boolean algebra originated theoretical computer science and, consequently, enabled the first digital computers to be built. Among other computer science fields, artificial intelligence was born. Various approaches towards automated reasoning emerged over the last few decades. While logical programming, ruled based systems and other ??formal?? paradigms proved successfull in solving multiple well defined tasks, they rendered useless when solving tasks with raw real world input or fuzzy data. SVM??. With soaring computational capabilities machine learning became state of the art.

