\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

In our work, we covered the generation of adversarial examples for image classifiers, using fast gradient sign method in white-box and surrogate attack scenario, along with black-box adversarial attack with our proposed genetic algortihm based solution. We assessed the performance of those methods in terms of success rate and computational complexity.

We carried out a multitude of experiments on publicly available Fashion MNIST dataset, which provides moderately difficult image classification task. With the emphasis on obtaining unbiased and valid observations, we employed model cross-validation and adversarial attack testing on significant amount of examples. The possible discrepancy in attack efficiency between various targeted architectures was assessed using two different architectures, both performing well on the original task. Under those test conditions, we got following results.

White-box fast gradient sign method attack yields perfect results, as it exploits the full knowledge of model internal workings, mainly inherent property od deep neural networks -- differentiability. While very successful, this type of attack is unfortunately the most impractical for the need to have full knowledge of targeted model -- unlikely case for embedded systems or cloud services. (TODO Just assumption not result) On the other hand, white-box fast gradient sign method attack may prove useful during training phase to enhance the adversarial attack resilience of the trained model.

We have carried out four types of surrogate fast gradient sign method attack, all successful to some extent. Non-targeted attack proved to be successful in most scenarios, while targeted attack proved to be substantialy harder and in terms of success rate, heavily souce -- targeted class dependent. The assumption of binary surrogate attack having significant performance gain over all other surrogate methods proved to be wrong, infact, binary surrogate attack even failed to outperform plain surrogate attack. Simple wide convolutional network (SimpleNet) proved to be more resilient than state of the art deep and narrow DenseNet architecture with residual connections. We account that behaviour to the extensive feature reuse in DenseNet, allowing for confusion of the model with weaker adversary. Surrogate attacks proved to be promising attack strategy, being moderately successful, while not having to have the full knowledge of the targeted model. Nevertheless, the adversary must have substantial expert knowledge to build surrogate model comparable to the targeted one and moderate amount of training data to approximate the target model output distribution.

The black-box adversarial attack was carried out using our genetic algorithm based solution. The attack yields perfect results in non-targeted scenario. In targeted scenario, compared to full same-architecture surrogate attack, it reports worsened results in case of SimpleNet, on the other hand nearly perfect results in case of DenseNet -- even outperforming the surrogate attack performance.

For the future work, we see the potential in extending our solution by informed operator based on surrogate model, hence creating hybrid approach. Such newly defined operator is expected to work as a mutation operator, using surrogate model and corresponding gradients obtained with fast gradient sign method. We recognize two possible origins for the surrogate model. Firstly, a \emph{pretrained} surrogate model may be provided during the initialization of the operator -- this approach may be suitable for underperforming surrogates to boost their performance with the help of genetic algorithm. Secondly, the surrogate model can be trained on the fly using targeted model predictions obtained from evaluation of objective function. We assume the surrogate model to get to estimate the gradients reliably after few epochs, as the training data i.e., the querries to targeted model are localized in small neighbourhood of the population of genetic algorithm. Using the FGSM mutation operator, the surrogate model may be exploited further as a substitute to targeted model in case of evaluation of objective function. We expect setting the policy of evaluation and training of the surrogate model not to be an easy task, which may even prove to be impractical for having too many hyperparameters sensitive to the correct settings.

We believe that appart from the study of adversarial attacks, study of defenses against those is just as important, if not more. We assume, that improvements in dataset augmentation as well as employing methods of adversarial attacks to the process of training may lead to significant improvements in resilience of deep learning models. Appart from that, study of feature reuse and its consequences with respect to adversarial attacks may offer interesting insights.

TODO In conclusion, we see what??
