\chapter{Adversarial Examples for~Deep Learning Models}
\label{sec:advex}
Performance of deep learning models is not perfect and certain amount of input data gets missclassified naturally due to the insufficient accuracy of models. However \cite{DBLP:journals/corr/SzegedyZSBEGF13} showed that misclassification can be achieved artificially by adding small perturbations designed to fool the model to the previously correctly classified examples. Those inputs are called \emph{adversarial examples}. We will focus on adversarial examples for image classification task.

The task of generating adversarial example for input $x$ and model $M_\theta$, such that $M_\theta(x) = l$ is the correct label, lies in finding the perturbation $\eta$, such that $M_\theta(x + \eta) = M_\theta(x') = l'$, with $l'$ being target class for the adversarial attack.

\section{Taxonomy}
\label{sec:advex_taxonomy}
We can characterise adversarial attacks by the following, mutually independent properties i.e., \emph{specificity, scope, adversary's knowledge} and \emph{perturbation constraints}.

The \emph{specifity} of adversarial attack is defined by the target class $l'$. In case of \emph{targeted} attack, the target class $l'$ corresponds to one specific classification category. On the other hand, in case of \emph{non-targeted} attack, $l'$ corresponds to all but the ground truth class $l$ and adversarial example is than any $x' = x + \eta$ such that $M_\theta(x') \neq l$.

The \emph{scope} of adversarial attack lies in number of original examples to be attacked by single $\eta$. \emph{Singleton} attack stands for attack on single example $x$ such that $M_\theta(x + \eta) = l'$. \emph{Multi} attack stands for attack on arbitrary number of examples  $X$ of the same class such that $\forall x \in X: M_\theta(x + \eta) = l'$. Finally, \emph{universal} attack seeks single $\eta$ such that $\forall x: M_\theta(x) = l \implies M_\theta(x + \eta) = l'$

We distinguish between three cases of \emph{adversary's knowledge}. The \emph{white-box attack} assumes full knowledge of targeted model i.e., architecture, weights and training data as well as parameters of the optimizer being used for training. On the other hand, the \emph{black-box attack} assumes no additional information about targeted model, appart from the classification predictions and respective probabilities. Finally, the \emph{surrogate attack} treats the targeted model as a black-box, yet has full access to substitute model i.e., model sharing some characteristics of target model e.g., task it solves, training data, architecture, output probability distribution, etc.

Regarding the \emph{perturbation constraints} of the attack, we distinguish the following three cases. \emph{Unconstrained} attack with no limitations on perturbation $\eta$, \emph{constrained} attack, with limited intensity of perturbation $\eta$ allowed, with respect to some measure, and finally the \emph{optimized} one, where the perturbation intensity is minimized. The $l1$ and $l2$ norms, or \emph{PASS} by \cite{DBLP:journals/corr/RozsaRB16} and \emph{SSIM} by \cite{ssim} -- visual similarity measures, are frequently used intensity measures.

\section{Gradient Based Methods}
\label{sec:advex_gbm}
Thanks to the inherent property of deep learning models -- differentiability, gradient based methods are easy to be used and perform well in white-box and reasonably well in surrogate scenarios. More of those methods have been invented, namely the \emph{L-BFGS} using Broyden–Fletcher–Goldfarb–Shanno optimization algorithm, fast gradient sign method and its modifications, feature adversary, one-pixel attack, etc. See \cite{DBLP:journals/corr/abs-1712-07107} for overview of adversarial attack approaches.

\paragraph{Fast Gradient Sign Method}
\label{sec:fgsm}
The fast gradient sign method (FGSM) computes gradients of loss function of the target model  with respect to the target example $x$ and class $l'$. The sign of gradients is taken and multiplied by $\epsilon$, the step size, and subtracted from $x$ (targeted attack) -- this way we move the target image in direction of rising target class prediction (\ref{eqn:fgsm}). In case of non-targeted attack, gradients are computed with respect to source class $l$ and, instead of substraction, we use addition to move the target image in direction of falling source class prediction.

\begin{equation} \label{eqn:fgsm}
x' = x - \epsilon \text{sgn}\left(\nabla_\theta L\left(M_\theta(x), l'\right)\right)
\end{equation}

\noindent The FGSM can be used iteratively with $x_t = x'_{t-1}$. Optional clipping after each step may take place in case of constrained attack. Versions of FGSM with momentum exists -- generating the adversarial example the same way as the training of deep learning models optimizes the parameters.

\section{Generative Models}
\label{sec:advex_gan}
Appart from methods based on backpropagation, usage of deep generative models for adversarial example generation has been researched. \cite{gan-advex} trained generative adversarial network (generator $G$) on target model task dataset, mapping random latent space to images and invertor $I$, mapping images to latent space of generator $G$. The adversarial examples are generated by finding $z'$ close to $z = I(x)$, such that $M_\theta\left(G(z')\right) = l'$. This approach genererates adversarial examples more natural to human observer as we are perturbing the hidden representation rather than the final output.
