\chapter{Adversarial Examples for Deep Learning Models}
Performance of deep learning models is not perfect and certain amount of input data gets missclassified naturally due to the insufficient accuracy of models. However \cite{DBLP:journals/corr/SzegedyZSBEGF13} showed that misclassification can be achieved artificially by adding small perturbations designed to fool the model to the previously correctly classified examples. Those inputs are called \emph{adversarial examples}. We will focus on adversarial examples for image classification task.
The task of generating adversarial example for input $x$, model $M_\theta$, such that $M_\theta(x) = l$ is the correct label, lies in finding the perturbation $\eta$, such that $M_\theta(x + \eta) = M_\theta(x') = l'$, with $l'$ being target class for the adversarial attack.

\section{Taxonomy}
We can characterise adversarial attacks by following, mutually independent, properties i.e., \emph{specificity, scope, adversary's knowledge} and \emph{perturbation constraints}.

The \emph{specifity} of adversarial attack is defined by the target class $l'$. In case of \emph{targeted} attack, the target class $l'$ corresponds to one specific classification category. On the other hand, in case of \emph{non-targeted} attack, $l'$ corresponds to all but the ground truth class $l$ and adversarial example is than any $x' = x + \eta$ such that $M_\theta(x') \neq l$.

The \emph{scope} of adversarial attack lies in number of original examples to be attacked by single $\eta$. \emph{Singleton} attack stands for attack on single example $x$ such that $M_\theta(x + \eta) = l'$. \emph{Multi} attack stands for attack on arbitrary number of examples  $x \in X$ of the same class such that $\forall x \in X: M_\theta(x + \eta) = l'$. Finally, \emph{universal} attack seeks single $\eta$ such that $\forall x: M_\theta(x) = l \implies M_\theta(x + \eta) = l'$

We distinguish between three cases of \emph{adversary's knowledge}. \emph{White-box} attack assumes full knowledge of targeted model i.e., architecture, weights and training data as well as parameters of the optimizer being used for training. On the other hand \emph{black-box} attack assumes no additional information about targeted model, appart from the classification predictions and respective probabilities. Finally \emph{Surrogate} attack treats the targeted model as a black-box, yet has full access to substitute model i.e., model sharing some characteristics of targe model e.g., task it solves, training data, architecture, output probability distribution, etc.

Regarding the \emph{perturbation constraints} of the attack, we distinguish the following three cases. \emph{Unconstrained} attack with no limitations on perturbation $\eta$, \emph{constrained} attack, with limited intensity of perturbation $\eta$ allowed, with respect to some measure, and finally \emph{optimized}, where the perturbetion intensity is minimized. Frequently used intensity measures are $l1$ and $l2$ norms, or \emph{PASS} by \cite{DBLP:journals/corr/RozsaRB16} and \emph{SSIM} by \cite{ssim} -- visual similarity measures.

\section{Gradient based methods}
Thanks to inherent property of deep learning models -- differentiability, gradient based methods are easy to be used and perform well in white-box and reasonably well in surrogate scenarios. More of those methods have been invented, namely \textit{L-BFGS} using Broyden–Fletcher–Goldfarb–Shanno optimization algorithm, fast gradient sign method and its modifications, feature adversary, one-pixel attack, etc.

\paragraph{Fast Gradient Sign Method}
The fast gradient sign method computes gradients of loss function of the target model  with respect to target example $x$ and class $l'$. The $sgn$ of gradients is taken and multiplied by $\epsilon$, the step size and subtracted from $x$ (targeted attack) -- this way we move the target image in direction of rising target class prediction (\ref{eqn:fgsm}). In case of non-targeted attack, gradients are computed with respect to source class $l$ and instead of subtraction we use adition to move the target image in direction of falling source class prediction.

\begin{equation} \label{eqn:fgsm}
x' = x - \epsilon sgn(\nabla_\theta L(M_\theta(x), l'))
\end{equation}

The FGSM can be used iteratively with $x_t = x'_{t-1}$. Optional clipping after each step may take place in case of constrained attack. Versions of FGSM with momentum exists -- generating the adversarial example the same way as the training of deep learning models optimizes the parameters.

\section{Generative models}
Appart from methods based on backpropagation, usage of deep generative models for adversarial example generation has been researched. \cite{gan-advex} trained generative adversarial network on task dataset mapping random latent space to images -- generator $G$ and invertor mapping images to latent space of generator $I$ -- inverter. The adversarial examples are generated by finding $z'$ close to $z = I(x)$, such that $M_\theta(G(z')) = l'$. This approach genererates adversarial examples more natural to human observer as we are perturbing the hidden representation and not the final output.
