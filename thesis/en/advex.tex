\chapter{Adversarial Examples for Deep Learning Models}
Deep learning models are not perfect and certain amount of input data gets missclassified naturally due to the insufficient accuracy of models; however \cite{DBLP:journals/corr/SzegedyZSBEGF13} showed that misclassification can be achieved artificially by adding small perturbations designed to fool the model to the previously correctly classified examples. Those inputs are called adversarial examples. We will focus on adversarial examples for image classification task. The task of generating adversarial example for input $x$, model $M_\theta$, such that $M_\theta(x) = l$ is the correct label, lies in finding the perturbation $\eta$, such that $M_\theta(x + \eta) = M_\theta(x') = l'$, with $l'$ being target class for the adversarial attack.

\section{Taxonomy}
We can characterise adversarial attacks by following, mutually independent, properties
\begin{itemize}
\item type of attack target
    \begin{description}
    \item[targeted] for $l'$ is preset to given class
    \item[non-targeted] if it holds that$l' \neq l$
    \end{description}
\item number of original examples to be attacked by single $\eta$
    \begin{description}
    \item[singleton] attack for single $x$ such that $M_\theta(x + \eta) = l'$
    \item[multi] attack for multiple $x \in X$ such that $\forall x \in X: M_\theta(x + \eta) = l'$
    \item[universal] attack for class $l$ such that $\forall x: M_\theta(x) = l \implies M_\theta(x + \eta) = l'$
    \end{description}
\item the amount of knowledge of the targeted model
    \begin{description}
    \item[white-box] attack, where the adversary has full (read-only) access to model architecture and parameters
    \item[black-box] attack, where the adversary has access only to the classification outputs (predicted class, scores)
    \item[surrogate] attack, where the adversary treats target model as a black-box, yet has full access to substitute model i.e., model sharing some characteristics of target model e.g., task, training data, architecture, output probability distribution, etc.
    \end{description}
\item intensity of perturbation with respect to given measure -- often $l1$ or $l2$ norm, or \textit{PASS} (\cite{DBLP:journals/corr/RozsaRB16}) and \textit{SSIM} (\cite{ssim}) visual similarity measures
    \begin{description}
    \item[constrained] for perturbation fitting in given range with respect to some meassure 
    \item[optimized] with perturbarion intensity being minimized
    \end{description}
\end{itemize}

\section{Gradient based methods}
Thanks to inherent property of deep learning models -- differentiability, gradient based methods are easy to be used and perform well in white-box and reasonably well in surrogate scenarios. More of those methods have been invented, namely \textit{L-BFGS} using Broyden–Fletcher–Goldfarb–Shanno optimization algorithm, fast gradient sign method and its modifications, feature adversary, one-pixel attack, etc.

\paragraph{Fast Gradient Sign Method}
The fast gradient sign method computes gradients of loss function of the target model  with respect to target example $x$ and class $l'$. The $sgn$ of gradients is taken and multiplied by $\epsilon$, the step size and subtracted from $x$ (targeted attack) -- this way we move the target image in direction of rising target class prediction \ref{eqn:fgsm}. In case of non-targeted attack, gradients are computed with respect to source class $l$ and instead of subtraction we use adition to move the target image in direction of falling source class prediction.

\begin{equation} \label{eqn:fgsm}
x' = x - \epsilon sgn(\nabla_\theta L(M_\theta(x), l'))
\end{equation}

The FGSM can be used iteratively with $x_t = x'_{t-1}$. Optional clipping after each step may take place in case of constrained attack. Versions of FGSM with momentum exists -- generating the adversarial example the same way as the training of deep learning models optimizes the parameters.

\section{Generative models}
Appart from methods based on backpropagation, usage of deep generative models for adversarial example generation has been researched. \cite{gan-advex} trained generative adversarial network on task dataset mapping random latent space to images -- generator $G$ and invertor mapping images to latent space of generator $I$ -- inverter. The adversarial examples are generated by finding $z'$ close to $z = I(x)$, such that $M_\theta(G(z')) = l'$. This approach genererate adversarial examples more natural to human observer as we are perturbing the hidden representation and not the final output.
