\chapter{Experiments}
\label{sec:experiments}
In order to empirically assess the performance of proposed solutions we have carried out a multitude of experiments on the Fashion MNIST dataset. This particular dataset was chosen for it is simple enough to enable thorough examination of various scenarios using k-fold cross-validation and measurements of results on statistically significant number of examples (often whole Fashion MNIST test set), yet more complex then MNIST and the like (\ref{subsec:mnist_tasks}). Following the results of Fashion MNIST benchmark (\cite{FASHION_BENCH}) we have chosen two high scoring model architectures of varying nature -- namely \emph{SimpleNet} and \emph{DenseNet}.

\emph{SimpleNet} is a simple wide shallow network with three convolutional layers of depth 32, 64 and 128 respectively, and one hidden dense layer of size 128. Each convolutional layer uses square kernel of size $3 \times 3$ and stride 1 with \textit{same} padding and is followed by batch normalization and ReLU activation. Max pooling with kernel of size $2 \times 2$ and stride 2 is added in-between each pair of convolutions for subsampling. Flattened output of the last convolutional layer is followed by ReLU activated hidden dense layer and final output layer of size corresponding to the number of classes. Dropout with rate $0.3$ is added after each max pooling and in front of hidden and output dense layers. Model is trained with Adam optimizer (see section \ref{sec:adam}) with default (recommended) parameters for 128 epochs. The initial learning rate $0.001$ is multiplied by $\frac{1}{5}$ in $50\%$ and $75\%$ of training process. Whole model has approximately 900k trainable parameters. To the best of our knowledge the name \emph{SimpleNet} does not refer to any well-known model architecture and was chosen for easier distinction of used architectures when referring to them.

As a second architecture, we use the \emph{DenseNet-BC} of depth $52$ with growth rate $k=8$ and 0.5 compression factor specifically, representing deep narrow convolutional network. Dropout of rate 0.2 is used as proposed by authors, as well as recommended training parameters, i.e., SGD with Nesterov momentum of 0.9 and learning rate of 0.1, multiplied by $\frac{1}{10}$ in $50\%$ and $75\%$ of training process which takes, again, 128 epochs. The whole model has approximately 120k trainable parameters (see section \ref{sec:densenet}).

Both models use weight decay of $0.0001$ as an additional regularization method as well as standard input preprocessing - mean subtraction and standard deviation division with channel-wise precomputed moments on training data. Batch size of 128 is used during training and no early stopping method is employed.

\begin{figure}
\centering
\begin{tabular}{@{}c@{}}
    \includegraphics[width=.64\textwidth]{../img/training.pdf}
    \includegraphics[width=.3\textwidth]{../img/models_test.pdf} \\
    \includegraphics[width=.48\textwidth]{../img/simplenet_cm.pdf}
    \includegraphics[width=.48\textwidth]{../img/densenet_cm.pdf}  \\
\end{tabular}
\caption{Cross validated training results}
\label{fig:training}
\end{figure}

Training of both architectures is carried out using stratified 10-fold cross-validation resulting in 20 target models trained with varying random initialization on differing training sets. From the test set accuracy boxplot at fig \ref{fig:training}, we can see that DenseNet outperforms SimpleNet by approximately $0.8\%$.

The confusion matrices of median models are very similar for both architectures. We can see that the \emph{shirt} class gets missclassified the most as it scores lowest accuracy of $\sim 83\%$, often confused with \emph{T-shirt/top} ($\sim 6\%$), then \emph{coat, pullover} and \emph{dress} (error rate of $2-5\%$). Appart from that, to some degree, the confusion matrices report symmetry -- showing that most pairs of classes, if confused, are confused mutually. Detailed observation hints the correspondence between confusion and visual similarity e.g., footwear or clothing categories.

\paragraph{Experiments overview}

In the following sections we are going to examine the following approaches towards generating adversarial examples for image classifiers, and we will measure their performance on the Fashion MNIST dataset.

Firstly, the white-box fast gradient sign method attack serving as both implementation sanity check and baseline is carried out (see section \ref{sec:whitebox_fgsm}). Next the transferability of adversarial examples is examined using surrogate attacks (see section \ref{sec:surrogate_fgsm}). Those approaches include transfering adversarial examples between models trained on the same dataset -- the most naive approach, training surrogate model on outputs of the target model, and finally training focused surrogate models as a binary classifiers for specific classification category in one-vs-all manner. For each approach based on the use of surrogate models, the transferability of adversarial examples between models of the same architecture as well as models of differing architectures is assessed. Appart from pure gradient methods, genetic algorithms are employed as a space search method, generating adversarial examples for the target model (see section \ref{sec:blackbox_ga}).

For some experiments one SimpleNet and one DenseNet model is chosen, taking median models with respect to accuracy. We will refer to them as \emph{median SimpleNet/DenseNet}. In the same manner, a single class is chosen, median in terms of model prediction accuracy and attack performance, for use in some experiments -- this proves to be the \emph{dress} class for Fashion MNIST dataset (see fig \ref{fig:training}).

Each approach is tested with either non-targeted and targeted singleton attack to each class. The attacks caried out are constrained to a maximum of 0.1 pixel-wise difference between targeted and adversarial examples, which corresponds approximately to 26 shades of gray difference in either direction. In case of the iterative FGSM (see section \ref{sec:fgsm}), step size of $\frac{1}{255}$ is used, which corresponds to shift by one shade of grey in the resulting adversarial example. The following experiments compare adversarial attack performance for various model architectures, target model knowledge available to adversary and characteristics of the attack itself.

\section{White-Box Attack}
\label{sec:whitebox_fgsm}
The white-box iterative FGSM is run against the target models directly. The following box plots (fig \ref{fig:fgsm_white_box}) show the distribution of the number of steps (\emph{y-axis}) needed to be taken to obtain adversarial example from source label (\emph{x-axis}) to target (\emph{plot title}), those results are collected on the whole Fashion MNIST test set using cross-validated target models i.e., for each class -- architecture pair, the median step count for each test set example is taken, resulting in $1000$ datapoints. Only valid datapoints i.e., those representing successfully generated adversarial examples are used to render each box. Presented heatmaps then show success rate of attack for each source -- target label pair for given architecture (\textit{plot title}). Blank cell in heatmap represents maximal possible value i.e., 1000.

\paragraph{Results}

Following from the results presented at fig \ref{fig:fgsm_white_box}, we can see that the white box attack against single example proves to be successful in both targeted and non-targeted scenarios for each examined architecture. SimpleNet architecture proves to be slightly more resilient both in terms of success rate and iterative FGSM step count. Consulting confusion matrix of targeted models, more confused classes are easier to attack in non-targeted scenario. In targeted attack the distribution of step count approximately corresponds to the distribution of targeted class confusion (column \textit{dress} in confusion matrices -- see fig \ref{fig:training}).

\begin{figure}
\centering
\begin{tabular}{@{}cc@{}}
    \includegraphics[width=.48\textwidth]{../img/experiments_fgsm_direct_non_targeted_single.pdf} &
    \includegraphics[width=.48\textwidth]{../img/experiments_fgsm_direct_simplenet_single_cm.pdf} \\
    \includegraphics[width=.48\textwidth]{../img/experiments_fgsm_direct_targeted_3_single.pdf}   &
    \includegraphics[width=.48\textwidth]{../img/experiments_fgsm_direct_densenet_single_cm.pdf}  \\
\end{tabular}
\caption{FGSM White-box}
\label{fig:fgsm_white_box}
\end{figure}

\section{Surrogate Attacks}
\label{sec:surrogate_fgsm}
The following experiment explores the transferability of adversarial examples using surrogate models. We compare the performance of following four scenarios: \emph{plain, full, reduced} and \emph{binary}.

\emph{Plain surrogate} model is trained on the same dataset (not necessarily using the same train, validation splitting) as the target model. \emph{Full surrogate} model is trained to fit output probability distribution of targeted model using the response of it on the whole training dataset. \emph{Reduced surrogate} is similar to full surrogate, except for being trained only on a fraction of training data to mimic limited ability to evaluate targeted model (when obtaining output probability distribution). We use $\frac{1}{10}$ of the training dataset i.e., 6000 examples. \emph{Binary surrogate} model is similar to full surrogate in terms of data size, but it is trained as a binary classifier between one chosen class and all other classes, allowing only targeted attack to selected class and non-targeted attack from it. We use the preselected median class i.e., \emph{dress}.

\paragraph{Results}
Appart from varying approaches towards surrogate training, we compare performance of surrogates being of the same architecture versus differing architecture from the targeted model. From attached plots for plain surrogate (fig \ref{fig:plain_surrogate}), full surrogate (fig \ref{fig:full_surrogate}), reduced surrogate (fig \ref{fig:reduced_surrogate}) and binary surrogate (fig \ref{fig:binary_surrogate}), we derive the following observations.

\begin{figure}
    \centering
    \includegraphics[width=.95\textwidth]{../img/experiments_fgsm_plain_surrogate__single__cm.pdf}
    \caption{Plain surrogate success rate}
    \label{fig:plain_surrogate}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.95\textwidth]{../img/experiments_fgsm_full_surrogate__single__cm.pdf}
    \caption{Full surrogate success rate}
    \label{fig:full_surrogate}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.95\textwidth]{../img/experiments_fgsm_full_surrogate_reduced__single__cm.pdf}
    \caption{Reduced surrogate success rate}
    \label{fig:reduced_surrogate}
\end{figure}

\begin{figure}
    \centering
    \begin{tabular}{@{}c@{}}
        \includegraphics[width=.24\textwidth]{../img/experiments_fgsm_binary_surrogate_simplenet_single_simplenet_cm.pdf}
        \includegraphics[width=.24\textwidth]{../img/experiments_fgsm_binary_surrogate_simplenet_single_densenet_cm.pdf}
        \includegraphics[width=.24\textwidth]{../img/experiments_fgsm_binary_surrogate_densenet_single_simplenet_cm.pdf}
        \includegraphics[width=.24\textwidth]{../img/experiments_fgsm_binary_surrogate_densenet_single_densenet_cm.pdf}  \\
    \end{tabular}
    \caption{Binary surrogate success rate}
    \label{fig:binary_surrogate}
\end{figure}

As expected, surrogate adversarial attack approaches perform inferior to direct white-box methods (see fig \ref{fig:fgsm_white_box}). In detail, full surrogate is the best surrogate approach (fig \ref{fig:full_surrogate}), followed by plain and binary surrogate (fig \ref{fig:plain_surrogate} and \ref{fig:binary_surrogate}), with reduced surrogate having the worst performance of all examined scenarios (see fig \ref{fig:reduced_surrogate}).

SimpleNet is more resilient to attacks than DenseNet (left column of heatmaps scoring lower values than right column). We assume that this might be caused by~extensive feature reuse in case of DenseNet (claimed by \cite{DBLP:journals/corr/HuangLW16a}) i.e., one channel of featuremap having significant role in prediction of multiple classes. Consequently, tampering with this shared, class independent feature, influence the prediction of multiple classes, shifting the models response. SimpleNet on the other hand may have more class dependent features, forcing the adversary to counter the evidence for prediction of the source class, while at the same time strenghten the prediction of targeted class.

We observe that it is easier to generate adversarial examples among visually similar classes -- types of shoes (\emph{sneaker, sandal, ankle boot}), clothes covering the chest (\emph{dress, coat, pullover, t-shirt/top}), while class unlike any other (\emph{trouser}) is resilient to adversarial attack. From this point of view, the \textit{bag} category might seem as an outlier, being very easily attacked, while not being type of footwear or clothing -- we think this might be again caused by feature reuse, as from the point of visual similarity, it is close to both of those groups.

The interesting phenomenon is the asymmetry in the performance of cross architecture attacks, with DenseNet target model attacked by SimpleNet surrogate being far more successful than its counterpart. This again may be caused by feature reuse. We think of it intuitively as of a surjective-only mapping between model features, with multiple SimpleNet features being mapped to one DenseNet feature. As a result, DenseNet is unable to attack specific SimpleNet features. Same architecture attack, on the other hand proves to be successful in both cases, outperforming cross-architecture attacks in all cases.

We notice comparable performance of plain surrogates, trained on the whole dataset, unaware of target model distribution; and reduced surrogates, trained on reduced dataset, aware of target model; both inferior to full surrogates, trained on the whole dataset, aware of target model. This confirms that both increasing amount of data and increasing knowledge about targeted model improve succes rate of surrogate adversarial attack.

Intristingly enough, the binary attack does not perform as well as we expected, being significantly inferior to full surrogate attack in case of SimpleNet attack, and sligtly inferior in case of DenseNet attack. Our assumption about binary model being by far the best, as it is trained to perfectly fit the boundary between target class and all other classes, proved to be wrong. We explain this by the shift in the task being solved by binary classifier, so that the gradients no longer follow the original full classifier.

\section{Black-Box Attack}
\label{sec:blackbox_ga}
We examine the performance of our proposed solution based on the use of genetic algorithms (see section \ref{sec:pga}).

The hyperparameters of the approach are set as follows: the cross-over takes place with probability $p_\text{xover} = 0.8$, the biased mutation uses binomial distribution with $n = 26, p = 0.5$ shifted to zero and scaled by factor $\frac{1}{255}$, with $p_\text{mut} = 0.3$ the probability of individual being mutated and $p_\text{gene} = 0.15$ the probability of gene mutation. The clipping bound of the perturbation $l2$ norm is set to $0.0005$ -- $0.95$ quantile of $l2$ norm of perturbations in FGSM attacks, for the sake of result comparability. Maximal adversarial example pixel-wise difference is constrained to $0.1$ to match the constraints of gradient based method experiments.

\paragraph{Results}
Regarding the justified increase in computational cost of running black-box attack, particularly genetic algorithm based, we test only two scenarios - median class targeted and non-targeted attack.

We compare the results with the best surrogate approach -- full, same architecture, fast gradient sign method surrogate. As showed in fig \ref{fig:ga_surrogate}, the performance of non-targeted attack is comparable using both approaches. On the other hand, the targeted attack exhibits significant assymetry regarding the different architectures. The observed assymetry follows the trend of SimpleNet being more resilient to adversarial attacks. SimpleNet black-box attack performance proves to be inferior to that of full surrogate attack on the same architecture while following similar distribution of source -- target pairs success rate. On the contrary, black-box attack targeted on DenseNet architecture yields perfect results, superior to those of surrogate attack, clearly showing the unexpected deviation in success rate. We assume this to be caused by genetic algorithm exploration capabilities, which the fast gradient sign method does not offer, sufficient to attack the weaker DenseNet model.

\begin{figure}
    \centering
    \begin{tabular}{@{}c@{}}
        \includegraphics[width=.24\textwidth]{../img/ea_simplenet_cm.pdf}
        \includegraphics[width=.24\textwidth]{../img/dress_experiments_fgsm_full_surrogate_simplenet_single_simplenet_cm.pdf}
        \includegraphics[width=.24\textwidth]{../img/ea_densenet_cm.pdf}
        \includegraphics[width=.24\textwidth]{../img/dress_experiments_fgsm_full_surrogate_densenet_single_densenet_cm.pdf}  \\
    \end{tabular}
    \caption{Black-box attack success rate (compared to full surrogate)}
    \label{fig:ga_surrogate}
\end{figure}

The black-box attack yields positive results in terms of number of targeted model evaluations (see fig \ref{fig:ga_steps}). The non-targeted attack on DenseNet architecture reports very low demands, with median complexity under $10\,000$ calls for each class. SimpleNet, again, proves to be more resilient, with median calls rising up to $30\,000$ with high variance. It is worth to note, that the distribution of steps needed to be taken to perform successful non-targeted attack against SimpleNet, loosely corresponds to the distribution of success rate of this attack. In case of targeted attack, namely attack targeted on \emph{dress} class, the difference in resiliency between model architectures is reported again. For DenseNet, targeted attack proves to be harder than non-targeted, rising the complexity up to $20\,000$ median calls for the \emph{bag} source class. SimpleNet attack complexity has increased by approximately the same offset, resulting in at most $40\,000$ median calls.

\section{Result Visual Comparison}
Figure \ref{fig:advex_vis} shows side-by-side comparison of obtained adversarial examples. From left to right (\emph{x-axis}), the following approaches are presented -- the original image, white-box FGSM attack, full surrogate same-architecture attack, and finally, the genetic algorithm based black-box attack. From top to bottom (\emph{y-axis}), the following scenarios are presented -- non-targeted SimpleNet and DenseNet attack, followed by \emph{dress} targeted SimpleNet and DenseNet attack. We can see that from the point of view of the human observer, adversarial examples do not hinder the ability to correctly deduce the source class. Moreover, in most cases, the perturbation is barely noticable.

\begin{figure}
    \centering
    \begin{tabular}{@{}c@{}}
        \includegraphics[width=.48\textwidth]{../img/ga_non_targeted.pdf}
        \includegraphics[width=.48\textwidth]{../img/ga_targeted_3.pdf}  \\
    \end{tabular}
    \caption{Number of black-box model evaluations (compared to full surrogate)}
    \label{fig:ga_steps}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../img/advex_vis.pdf}
    \caption{Comparison of successful adversarial examples}
    \label{fig:advex_vis}
\end{figure}
